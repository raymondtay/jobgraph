# For testing purposes only
akka.loglevel = DEBUG
akka.loggers = ["akka.testkit.TestEventListener"]

# Tell Jobgraph where Mesos master node(s) is so that we can hook to it.
# When enabled is "true", jobgraph will attempt to submit to the Mesos cluster
# and run the Google Dataflow runner; but if enabled is "false", then jobgraph
# will still run the Google Dataflow runner but only on the co-located host as
# jobgraph.
#
# note: MESOS_CONTACT_POINT=127.0.0.1:5050,10.148.0.2:5050 should work
#
mesos {
  enabled  : false
  runas    : hicoden
  timeout  : 10 # Note: this cannot be less than 5 seconds
  uris : ["10.148.0.7:5050", "10.148.0.7:5050"]
}

# Location of the JobGraph engine and HTTP port
# note: internal IP.
#
jobgraph {
  hostname : localhost
  hostport : 9000
}

jobgraph.db {
  driver   : "org.postgresql.Driver"
  name     : "jdbc:postgresql:circle_test",
  username : "root",
  password : "circle_test"
}

# For the purpose of testing - the formulations here are correct based on the
# Step specification

include "workflows.conf"

jobs : [
  {
    id : 0
    name : "99 beers on the wall."
    description : "99 beers on the wall."
    workdir : ${?HOME}"/working_dir"
    workdir : ${?JOB_WORK_DIR}
    sessionid : "TEST_ID_0"
    sessionid : ${?SESSION_ID}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Dataflow:python"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
  
  }
]
jobs2 : [
  {
    id : 1
    name : "98 beers on the wall"
    description : "98 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?JOB_WORK_DIR}
    sessionid: "TEST_ID_1"
    sessionid: ${?SESSION_ID}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Dataflow:java"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }

  }
]
jobs3 : [

  {
    id : 2
    name : "97 beers on the wall"
    description : "97 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?JOB_WORK_DIR}
    sessionid: "TEST_ID_2"
    sessionid: ${?SESSION_ID}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Dataflow:java"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
 
  },

  {
    id : 3
    name : "96 beers on the wall"
    description : "96 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?jobgraph.settings.workdir}
    sessionid: "TEST_ID_3"
    sessionid: ${jobgraph.settings.sessionid}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Dataflow:python"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
  
  }
]

jobs4 : [

  {
    id : 4
    name : "97 beers on the wall"
    description : "97 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?JOB_WORK_DIR}
    sessionid: "TEST_ID_2"
    sessionid: ${?SESSION_ID}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Flink:haskell"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
 
  },

  {
    id : 5
    name : "96 beers on the wall"
    description : "96 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?jobgraph.settings.workdir}
    sessionid: "TEST_ID_3"
    sessionid: ${jobgraph.settings.sessionid}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Dataflow:"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
  
  },

  {
    id : 6
    name : "96 beers on the wall"
    description : "96 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?jobgraph.settings.workdir}
    sessionid: "TEST_ID_3"
    sessionid: ${jobgraph.settings.sessionid}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : ":java"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
  
  },

  {
    id : 7
    name : "96 beers on the wall"
    description : "96 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?jobgraph.settings.workdir}
    sessionid: "TEST_ID_3"
    sessionid: ${jobgraph.settings.sessionid}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : "Dataflow:haskell"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
  
  },

  {
    id : 8
    name : "96 beers on the wall"
    description : "96 beers on the wall"
    workdir: ${?HOME}"/working_dir"
    workdir: ${?jobgraph.settings.workdir}
    sessionid: "TEST_ID_3"
    sessionid: ${jobgraph.settings.sessionid}
    restart.max = 2

    runner {
     module : "apache_beam.examples.wordcount"
     runner : ":"
     cliargs : ["--jobName bloody-google-dataflow",
                "--input gs://hicoden/README.md",
                "--output gs://hicoden-test-1/XX",
                "--runner DataflowRunner",
                "--project hicoden",
                "--temp_location gs://hicoden-test-1/",
                "--callback http://0.0.0.0:9000/flow/<wfId>/job/<jobId>"
               ]
    }
  
  },

]

workflows : [
  {
    id : 0
    name : "A sample process"
    description : "Reads from file, processed them and finally read them"
    steps : [0,1,2,3]
    jobgraph : [
      "0 -> 1",
      "0 -> 2",
      "2 -> 3",
      "1 -> 3"
    ]
  }
]

workflows2 : [
  {
    id : 1
    name : "Another sample process"
    description : "Reads from file, processed them and finally read them"
    steps : [0,1,2,3]
    jobgraph : [
      "0 -> 1",
      "0 -> 2",
      "2 -> 3",
      "1 -> 3"
    ]
  }
]

# This workflow is crafted with s.t. the jobgraph contains nodes not present in
# the system
workflows3 : [
  {
    id : 2
    name : "Another sample process"
    description : "Reads from file, processed them and finally read them"
    steps : [0,1,2,3]
    jobgraph : [
      "0 -> 1",
      "0 -> 2",
      "2 -> 3",
      "1 -> 9"
    ]
  }
]

